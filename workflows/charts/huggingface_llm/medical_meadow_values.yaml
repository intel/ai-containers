# Copyright (c) 2023 Intel Corporation
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0


metadata:
  name: medical-meadow
  namespace: kubeflow

secret:
  encodedToken:

image:
  name: intel/ai-workflows  # Specify the image name that was pushed to docker hub or copied to the nodes
  tag: torch-2.2.0-huggingface-multinode-py3.10  # Specify the image tag that was pushed to docker hub or copied to the nodes
  pullPolicy: IfNotPresent

securityContext:
  runAsUser:
  runAsGroup:
  fsGroup:
  allowPrivilegeEscalation: false

elasticPolicy:
  rdzvBackend: c10d
  minReplicas: 1
  maxReplicas: 4  # Must be greater than or equal to the number of distributed.workers
  maxRestarts: 30

distributed:
  workers: 4
  script: /workspace/scripts/finetune.py
  modelNameOrPath: meta-llama/Llama-2-7b-hf
  logLevel: info

  doTrain: True
  doEval: True
  doBenchmark: False
  doQuantize: False

  train:
    datasetName: medalpaca/medical_meadow_medical_flashcards  # Name of the Hugging Face dataset to use. Leave blank if using a data file
    dataFile:  # Path to a dataset file. Leave blank if using a Hugging Face dataset.
    datasetConcatenation: True
    promptWithInput: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.
    promptWithoutInput: Below is an instruction that describes a task. Write a response that appropriately completes the request.
    perDeviceBatchSize: 8
    epochs: 3
    maxSteps: -1
    gradientAccumulationSteps: 1
    learningRate: 2e-5
    ddpFindUnusedParameters: False
    ddpBackend: ccl
    useFastTokenizer: False
    outputDir: /tmp/pvc-mount/output/saved_model
    loggingSteps: 10
    saveTotalLimit: 2
    saveStrategy: epoch
    useLora: True
    loraRank: 8
    loraAlpha: 16
    loraDropout: 0.1
    loraTargetModules: q_proj vproj
    noCuda: True
    overwriteOutputDir: True
    bf16: True
    useIpex: True
  eval:
    perDeviceBatchSize: 8
    validationSplitPercentage: 0.2
  benchmark:
    warmup: 30
    iterations: 300
    coresPerInstance: -1
    numInstances: 1
  quantize:
    peftModelDir: /tmp/pvc-mount/output/saved_model  # If training, set this to the train.outputDir to quantize the trained model
    outputDir: /tmp/pvc-mount/output/quantized_model
    woqBits: 8
    woqGroupSize: -1
    woqScheme: sym
    woqAlgo: RTN

envVars:
  ldPreload: /usr/lib/x86_64-linux-gnu/libtcmalloc.so.4.5.9:/usr/local/lib/libiomp5.so
  logLevel: INFO
  transformersCache: /tmp/pvc-mount/transformers_cache
  hfDatasetsCache: /tmp/pvc-mount/hf_dataset_cache
  hfHome: /tmp/home
  cclWorkerCount: 1
  httpProxy:
  httpsProxy:
  noProxy:
  ftpProxy:
  socksProxy:

# Resources allocated to each worker
resources:
  cpuRequest:
  cpuLimit:
  memoryRequest:
  memoryLimit:
  nodeSelectorLabel:
  nodeSelectorValue:

# Persistent volume claim storage resources
storage:
  storageClassName: nfs-client
  resources: 50Gi
  pvcMountPath: /tmp/pvc-mount
