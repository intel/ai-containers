{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Samples to Get Started with AI Tools Selector Deep Learning Preset Container\n",
    "This Container features samples to get started with understanding how ai tools selector presets delivers optimized and scalable solutions for Deep Learning using Jupyterlab.\n",
    "\n",
    "Below are a list of samples included that take advantage of Intel Optimizations for a given AI Tool:\n",
    "1. [ResNet50 Inference with Intel速 Extension for PyTorch](./ipex/ResNet50_Inference.ipynb) : This code sample will guide users how to run a PyTorch inference workload on both GPU and CPU by using oneAPI AI Analytics Toolkit and also analyze the GPU and CPU usage via oneDNN verbose logs.\n",
    "2. [Training and Inference Optimization with Intel速 Extension for PyTorch](./ipex-inference/IntelPyTorch_GPU_InferenceOptimization_with_AMP.ipynb) : This code sample will train a ResNet50 model using the CIFAR10 dataset while using Intel速 Extension for PyTorch*. The model is trained using FP32 by default but can also be trained with AMP BF16 precision by passing BF16 parameter in the Train function. Then the same trained model is taken and inference with FP32 and AMP BF16 is done and latency is compared to see the performance improvement with the use of Intel速 Xe Matrix Extensions(XMX) for BF16. XMX is supported on BF16 and INT8 data types on Intel discrete GPUs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
