{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Samples to Get Started with Intel® AI Tools Selector Inference Optimization Preset Container\n",
    "This Container features several samples to get started with understanding how Intel® AI Tools delivers optimized and scalable solutions for Inference Optimization workflows using Jupyterlab.\n",
    "\n",
    "Below are a list of samples included that take advantage of Intel Optimizations for a given AI Tool:\n",
    "1. [Intel® Neural Compressor + PyTorch](./inc-ipex-quantization/quantize_with_inc.ipynb): \n",
    "In this notebook we will look at a real-world use case of text classification using a Huggingface model. We will first use a stock FP32 PyTorch model to generate predictions. Then, we will perform INT8 Quantization with easy-to-use APIs provided by INC to see how speedups can be gained over stock PyTorch on Intel® hardware\n",
    "2. [Intel® Neural Compressor + TensorFlow](./inc-itex/inc_sample_tensorflow.ipynb): In this notebook we will demonstrate how you can use Intel Neural Compressor to quantize a TensorFlow model. First we will train a simple Convolution Neural Network model in Keras TensoFlow on the MNIST dataset. Then we will use Intel Neural Compressor to quantize the saved checkpoint. We will then use the quantized model to compare the performance with the original trained model to demonstrate the improvement. The performance metrics that's used to test the quantized model is **latency** and **throughput** while keeping the **accuracy** close to the orignial model.\n",
    "3. [Intel® Extension for TensorFlow(ITEX)](./itex-inference/tutorial_optimize_TensorFlow_pretrained_model.ipynb): To get a good performance on your pre-trained model for inference, some inferece optimizations are required. \n",
    "This tutorial will guide you how to optimize a pre-trained model for a better inference performance, and also analyze the model pb files before and after the inference optimization.\n",
    "\n",
    "   Those optimizations includes:  \n",
    "    * Converting variables to constants.\n",
    "    * Removing training-only operations like checkpoint saving.\n",
    "    * Stripping out parts of the graph that are never reached.\n",
    "    * Removing debug operations like CheckNumerics.\n",
    "    * Folding batch normalization ops into the pre-calculated weights.\n",
    "    * Fusing common operations into unified versions.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
