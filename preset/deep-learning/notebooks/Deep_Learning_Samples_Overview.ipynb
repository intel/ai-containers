{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Samples to Get Started with Intel® AI Tools Selector Deep Learning Preset Container\n",
    "This Container features several samples to get started with understanding how Intel® AI Tools delivers optimized and scalable solutions for Deep Learning using Jupyterlab.\n",
    "\n",
    "Below are a list of samples included that take advantage of Intel Optimizations for a given AI Tool:\n",
    "1. [ResNet50 Inference with Intel® Extension for PyTorch](./ipex/ResNet50_Inference.ipynb) : This code sample will guide users how to run a PyTorch inference workload on both GPU and CPU by using oneAPI AI Analytics Toolkit and also analyze the GPU and CPU usage via oneDNN verbose logs.\n",
    "2. [Training and Inference Optimization with Intel® Extension for PyTorch](./ipex-inference/IntelPyTorch_GPU_InferenceOptimization_with_AMP.ipynb) : This code sample will train a ResNet50 model using the CIFAR10 dataset while using Intel® Extension for PyTorch*. The model is trained using FP32 by default but can also be trained with AMP BF16 precision by passing BF16 parameter in the Train function. Then the same trained model is taken and inference with FP32 and AMP BF16 is done and latency is compared to see the performance improvement with the use of Intel® Xe Matrix Extensions(XMX) for BF16. XMX is supported on BF16 and INT8 data types on Intel discrete GPUs.\n",
    "3. [Quantization with Intel® Extension for PyTorch](./ipex-quantization/IntelPytorch_Quantization.ipynb) : This code sample will quantize a ResNet50 model while using Intel® Extension for PyTorch (IPEX). The model will run inference with FP32 and INT8 precision, including static INT8 quantization and dynamic INT8 quantization. During Static Quantization, the model calibrated with the CIFAR10 dataset. The inference time will be compared, showcasing the speedup of INT8 Quantization.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
