classical-ml/docker-compose.yaml:        org.opencontainers.base.name: "intel/python:3.10-core"
classical-ml/README.md:    intel/intel-optimized-ml:2024.2.0-xgboost-2.0.3-pip-jupyter
preset/README.md:| Data Analytics | Perform large scale data analysis |[Modin*](https://github.com/modin-project/modin), [Intel® Dataset Librarian](https://github.com/IntelAI/models/tree/master/datasets/dataset_api), [Intel® Data Connector](https://github.com/IntelAI/models/tree/master/datasets/cloud_data_connector) | [`intel/data-analytics:latest-py3.9`](https://hub.docker.com/r/intel/data-analytics/tags)<br />[`intel/data-analytics:latest-py3.10`](https://hub.docker.com/r/intel/data-analytics/tags) |
preset/README.md:| Classical ML | Train classical-ml models using scikit, modin and xgboost |[Intel® extension for SciKit Learn](https://github.com/intel/scikit-learn-intelex), [Intel® Optimization for XGBoost*](https://github.com/dmlc/xgboost), [Modin*](https://github.com/modin-project/modin), <br /> [Intel® Dataset Librarian](https://github.com/IntelAI/models/tree/master/datasets/dataset_api), [Intel® Data Connector](https://github.com/IntelAI/models/tree/master/datasets/cloud_data_connector) | [`intel/classical-ml:latest-py3.9`](https://hub.docker.com/r/intel/classical-ml/tags)<br />[`intel/classical-ml:latest-py3.10`](https://hub.docker.com/r/intel/classical-ml/tags) |
preset/README.md:| Deep Learning | Train large scale Deep Learning models with Tensorflow or PyTorch | [Intel® Extension for PyTorch](https://github.com/intel/intel-extension-for-pytorch), [Intel® Extension for Tensorflow](https://github.com/intel/intel-extension-for-tensorflow),<br /> [Intel® Optimization for Horovod](https://github.com/intel/intel-optimization-for-horovod), [Intel® Dataset Librarian](https://github.com/IntelAI/models/tree/master/datasets/dataset_api), [Intel® Data Connector](https://github.com/IntelAI/models/tree/master/datasets/cloud_data_connector), [Intel® Extension for DeepSpeed](https://github.com/intel/intel-extension-for-deepspeed) | [`intel/deep-learning:latest-py3.9`](https://hub.docker.com/r/intel/deep-learning/tags)<br />[`intel/deep-learning:latest-py3.10`](https://hub.docker.com/r/intel/deep-learning/tags) |
preset/README.md:| Inference Optimization | Optimize Deep Learning models for inference<br /> using Intel® Neural Compressor | [Intel® Extension for PyTorch](https://github.com/intel/intel-extension-for-pytorch), [Intel® Extension for Tensorflow](https://github.com/intel/intel-extension-for-tensorflow),<br /> [Intel® Neural Compressor](https://github.com/intel/neural-compressor), [Intel® Dataset Librarian](https://github.com/IntelAI/models/tree/master/datasets/dataset_api), [Intel® Data Connector](https://github.com/IntelAI/models/tree/master/datasets/cloud_data_connector) | [`intel/inference-optimization:latest-py3.9`](https://hub.docker.com/r/intel/inference-optimization/tags)<br />[`intel/inference-optimization:latest-py3.10`](https://hub.docker.com/r/intel/inference-optimization/tags) |
preset/README.md:docker pull intel/deep-learning:latest-py3.9
preset/README.md:    intel/deep-learning:latest-py3.9 bash
preset/README.md:    intel/deep-learning:latest-py3.9 bash
preset/README.md:    intel/deep-learning:latest-py3.9
preset/README.md:    intel/deep-learning:latest-py3.9
preset/README.md:docker run ... intel/deep-learning:latest-py3.9 \
preset/deep-learning/demo/pytorch-distributed/README.md:        -w /tests intel/deep-learning:2024.0-py3.10 \
preset/deep-learning/demo/pytorch-distributed/README.md:    -it intel/deep-learning:2024.0-py3.10 conda run --no-capture-output \
preset/deep-learning/demo/pytorch-distributed/README.md:        -w /tests intel/deep-learning:2024.0-py3.10 \
preset/deep-learning/demo/pytorch-distributed/README.md:    -it intel/deep-learning:2024.0-py3.10 conda run --no-capture-output \
preset/deep-learning/demo/tensorflow-distributed/README.md:        -w /tests intel/deep-learning:2024.0-py3.10 \
preset/deep-learning/demo/tensorflow-distributed/README.md:        -w /tests intel/deep-learning:2024.0-py3.10 \
preset/deep-learning/demo/tensorflow-distributed/README.md:    -w /tests intel/deep-learning:2024.0-py3.10 \
preset/deep-learning/demo/tensorflow-distributed/README.md:    -w /tests intel/deep-learning:2024.0-py3.10 \
pytorch/serving/README.md:           intel/intel-optimized-pytorch:2.4.0-serving-cpu \
pytorch/serving/README.md:          intel/intel-optimized-pytorch:2.4.0-serving-cpu
pytorch/serving/README.md:              intel/intel-optimized-pytorch:2.4.0-serving-cpu
pytorch/serving/kfs.patch:+DOCKER_TAG="intel/torchserve:latest-kfs"
pytorch/serving/kfs.patch:+BASE_IMAGE="intel/torchserve:latest"
pytorch/serving/build-kfs.sh:docker tag "${REGISTRY}/${REPO}:b-${GITHUB_RUN_NUMBER:-0}-ubuntu-22.04-py3.10-torchserve" intel/torchserve:latest
pytorch/serving/patch.yaml:      image: "intel/intel-extension-for-tensorflow:serving-cpu"
pytorch/serving/patch.yaml:      image: "intel/intel-extension-for-pytorch:2.4.0-serving-cpu-kserve"
pytorch/README.md:    intel/intel-extension-for-pytorch:2.1.40-xpu
pytorch/README.md:    intel/intel-extension-for-pytorch:2.1.40-xpu-pip-jupyter
pytorch/README.md:docker run -it --rm intel/intel-extension-for-pytorch:latest
pytorch/README.md:    intel/intel-extension-for-pytorch:2.4.0-pip-jupyter
pytorch/README.md:            intel/intel-extension-for-pytorch:2.4.0-pip-multinode \
pytorch/README.md:            intel/intel-extension-for-pytorch:2.4.0-pip-multinode \
pytorch/README.md:    intel/intel-extension-for-pytorch:2.4.0-pip-multinode \
pytorch/README.md:    intel/intel-extension-for-pytorch:2.4.0-pip-multinode-hf-4.44.0-genai \
pytorch/docker-compose.yaml:        org.opencontainers.base.name: "intel/python:3.10-core"
pytorch/docker-compose.yaml:        org.opencontainers.base.name: "intel/python:3.10-core"
pytorch/docker-compose.yaml:        org.opencontainers.base.name: "intel/python:3.10-core"
site/tensorflow/index.html:<span class="w">    </span>intel/intel-extension-for-tensorflow:xpu
site/tensorflow/index.html:<span class="w">    </span>intel/intel-extension-for-tensorflow:xpu-jupyter
site/tensorflow/index.html:<span class="w">    </span>intel/intel-extension-for-tensorflow:serving-gpu
site/tensorflow/index.html:<span class="w">    </span>intel/intel-extension-for-tensorflow:xpu-jupyter
site/tensorflow/index.html:<span class="w">    </span>intel/intel-extension-for-tensorflow:serving-cpu
site/search/search_index.json:{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Intel\u00ae AI Containers","text":"<p>This repository contains Dockerfiles, scripts, yaml files, Helm charts, etc. used to scale out AI containers with versions of TensorFlow and PyTorch that have been optimized for Intel platforms. Scaling is done with python, Docker, kubernetes, kubeflow, cnvrg.io, Helm, and other container orchestration frameworks for use in the cloud and on-premise.</p>"},{"location":"#project-setup","title":"Project Setup","text":"<p>Define your project's registry and repository each time you use the project:</p> <pre><code># REGISTRY/REPO:TAG\nexport CACHE_REGISTRY=&lt;cache_registry_name&gt;\nexport REGISTRY=&lt;registry_name&gt;\nexport REPO=&lt;repo_name&gt;\n</code></pre> <p>Note</p> <p><code>REGISTRY</code> and <code>REPO</code> are used to authenticate with the private registry necessary to push completed container layers and saved them for testing and publication. For example: <code>REGISTRY=intel &amp;&amp; REPO=intel-extension-for-pytorch</code> would become <code>intel/intel-extension-for-pytorch</code> as the name of the container image, followed by the tag generated from the service found in that project's compose file.</p>"},{"location":"#set-up-docker-engine","title":"Set Up Docker Engine","text":"<p>You'll need to install Docker Engine on your development system. Note that while Docker Engine is free to use, Docker Desktop may require you to purchase a license.  See the Docker Engine Server installation instructions for details.</p>"},{"location":"#set-up-docker-compose","title":"Set Up Docker Compose","text":"<p>Ensure you have Docker Compose installed on your machine. If you don't have this tool installed, consult the official Docker Compose installation documentation.</p> <pre><code>DOCKER_CONFIG=${DOCKER_CONFIG:-$HOME/.docker}\nmkdir -p $DOCKER_CONFIG/cli-plugins\ncurl -SL https://github.com/docker/compose/releases/download/v2.26.1/docker-compose-linux-x86_64 -o $DOCKER_CONFIG/cli-plugins/docker-compose\nchmod +x $DOCKER_CONFIG/cli-plugins/docker-compose\ndocker compose version\n</code></pre> <p>Warning</p> <p>Docker compose <code>v2.25.0</code> is the minimum required version for some container groups.</p>"},{"location":"#build-containers","title":"Build Containers","text":"<p>Select your framework of choice (TensorFlow, PyTorch, Classical ML) and run the docker compose commands:</p> <pre><code>cd &lt;framework&gt;\ndocker compose up --build\n</code></pre> <p>To configure these containers, simply append the relevant environment variable to the docker compose command based on the build arguments in the compose file. For example:</p> <pre><code># I want to build ipex-base with Intel\u00ae Distribution for Python\ncd pytorch\nPACKAGE_OPTION=idp docker compose up --build ipex-base\n</code></pre>"},{"location":"#test-containers","title":"Test Containers","text":"<p>To test the containers, use the Test Runner Framework:</p> <pre><code># I want to test ipex-base with Intel\u00ae Distribution for Python\n# 1. build the container in the above section\n# 2. push it to a relevant registry\nPACKAGE_OPTION=idp docker compose push ipex-base\ncd ..\n# 3. install the test runner python requirements\npip install -r test-runner/requirements.txt\n# 4. Run the test file\nPACKAGE_OPTION=idp python test-runner/test_runner.py -f pytorch/tests/tests.yaml\n</code></pre> <p>Tip</p> <p>To test a container built by GitHub Actions CI/CD, find the <code>run number</code> associated with the workflow run and set the <code>GITHUB_RUN_NUMBER</code> environment variable during execution to pull the desired image.</p>"},{"location":"#troubleshooting","title":"Troubleshooting","text":"<ul> <li>See the Docker Troubleshooting Article.</li> <li>Verify that Docker Engine Post-Install Steps are completed.</li> <li>When facing socket error check the group membership of the user and ensure they are part of the <code>docker</code> group.</li> <li>After changing any docker files or configs, restart the docker service <code>sudo systemctl restart docker</code>.</li> <li>Enable Docker Desktop for WSL 2.</li> <li>If you are trying to access a container UI from the browser, make sure you have port forwarded and reconnect.</li> <li>If your environment requires a proxy to access the internet, export your development system's proxy settings to the docker environment:</li> </ul> <pre><code>export DOCKER_BUILD_ARGS=\"--build-arg ftp_proxy=${ftp_proxy} \\\n  --build-arg FTP_PROXY=${FTP_PROXY} --build-arg http_proxy=${http_proxy} \\\n  --build-arg HTTP_PROXY=${HTTP_PROXY} --build-arg https_proxy=${https_proxy} \\\n  --build-arg HTTPS_PROXY=${HTTPS_PROXY} --build-arg no_proxy=${no_proxy} \\\n  --build-arg NO_PROXY=${NO_PROXY} --build-arg socks_proxy=${socks_proxy} \\\n  --build-arg SOCKS_PROXY=${SOCKS_PROXY}\"\n</code></pre> <pre><code>export DOCKER_RUN_ENVS=\"-e ftp_proxy=${ftp_proxy} \\\n  -e FTP_PROXY=${FTP_PROXY} -e http_proxy=${http_proxy} \\\n  -e HTTP_PROXY=${HTTP_PROXY} -e https_proxy=${https_proxy} \\\n  -e HTTPS_PROXY=${HTTPS_PROXY} -e no_proxy=${no_proxy} \\\n  -e NO_PROXY=${NO_PROXY} -e socks_proxy=${socks_proxy} \\\n  -e SOCKS_PROXY=${SOCKS_PROXY}\"\n</code></pre> <pre><code>docker build $DOCKER_BUILD_ARGS -t my:tag .\ndocker run $DOCKER_RUN_ENVS --rm -it my:tag\n</code></pre>"},{"location":"#support","title":"Support","text":"<p>The Intel AI MLOps team tracks bugs and enhancement requests using GitHub issues. Before submitting a suggestion or bug report, search the existing GitHub issues to see if your issue has already been reported.</p> <ul> <li>Trademarks</li> </ul>"},{"location":"matrix/","title":"Support Matrix","text":"Framework ContainersModel ContainersPreset ContainersOther PythonClassical MLPyTorchTensorFlow Idp Version full core Container Intel\u00ae Distribution for Python Base Image Intel\u00ae Distribution for Python Base Image Image Name intel/python:3.10-full intel/python:3.10-core Base Image Name ubuntu:22.04 ubuntu:22.04 Python Version 3.10 3.10 OS Dependencies wget wget Python Dependencies numpy 1.26.4 numpy 1.26.4 setuptools 69.5.1 setuptools 69.5.1 psutil 5.9.8 psutil 5.9.8 mkl 2024.1.0 mkl 2024.1.0 mkl-include 2024.1.0 mkl-include 2024.1.0 intel-openmp 2024.1.0 intel-openmp 2024.1.0 Conda Dependencies intel-distribution-for-python intelpython3_full intel-distribution-for-python intelpython3_core <p>{{ Cannot find 'assets/classical-ml.csv' }}</p> Package Option idp pip Container Intel\u00ae Extension for PyTorch Base Image Intel\u00ae Extension for PyTorch Base Image Image Name intel/intel-optimized-pytorch:2.2.0-idp-base intel/intel-optimized-pytorch:2.2.0-pip-base Base Image Name intel/python:3.10-core intel/python:3.10-core Python Version 3.10 3.10 OS Dependencies Python Dependencies torch 2.3.0 torch 2.3.0 torchvision 0.18.0 torchvision 0.18.0 torchaudio 2.3.0 torchaudio 2.3.0 intel_extension_for_pytorch 2.3.0+cpu intel_extension_for_pytorch 2.3.0+cpu Conda Dependencies Container Intel\u00ae Extension for PyTorch Jupyter Image Intel\u00ae Extension for PyTorch Jupyter Image Image Name intel/intel-optimized-pytorch:2.2.0-idp-jupyter intel/intel-optimized-pytorch:2.2.0-pip-jupyter Base Image Name intel/intel-optimized-pytorch:2.2.0-idp-base intel/intel-optimized-pytorch:2.2.0-pip-base Python Version 3.10 3.10 OS Dependencies Python Dependencies jupyterlab 4.2.0b1 jupyterlab 4.2.0b1 jupyterhub 4.1.5 jupyterhub 4.1.5 notebook 7.2.0b0 notebook 7.2.0b0 jupyter-server-proxy 4.1.2 jupyter-server-proxy 4.1.2 Conda Dependencies Container Intel\u00ae Extension for PyTorch MultiNode Image Intel\u00ae Extension for PyTorch MultiNode Image Image Name intel/intel-optimized-pytorch:2.2.0-idp-multinode intel/intel-optimized-pytorch:2.2.0-pip-multinode Base Image Name intel/intel-optimized-pytorch:2.2.0-idp-base intel/intel-optimized-pytorch:2.2.0-pip-base Python Version 3.10 3.10 OS Dependencies gcc gcc libgl1-mesa-glx libgl1-mesa-glx libglib2 libglib2 python3-dev python3-dev virtualenv Python Dependencies oneccl_bind_pt 2.3.0+cpu oneccl_bind_pt 2.3.0+cpu neural-compressor 2.5.1 neural-compressor 2.5.1 Conda Dependencies Container Intel\u00ae Extension for PyTorch XPU Base Image Intel\u00ae Extension for PyTorch XPU Base Image Image Name intel/intel-optimized-pytorch:2.1.20-xpu-idp-base intel/intel-optimized-pytorch:2.1.20-xpu-pip-base Base Image Name intel/python:3.10-core intel/python:3.10-core Python Version 3.10 3.10 OS Dependencies build-essential build-essential clinfo clinfo git git gnupg2 gnupg2 gpg-agent gpg-agent intel-level-zero-gpu 1.3.27642.40-803~22.04 intel-level-zero-gpu 1.3.27642.40-803~22.04 intel-oneapi-runtime-ccl 2021.12.0-309 intel-oneapi-runtime-ccl 2021.12.0-309 intel-oneapi-runtime-dpcpp-cpp 2024.1.0-963 intel-oneapi-runtime-dpcpp-cpp 2024.1.0-963 intel-oneapi-runtime-mkl 2024.1.0-691 intel-oneapi-runtime-mkl 2024.1.0-691 intel-opencl-icd 23.43.27642.40-803~22.04 intel-opencl-icd 23.43.27642.40-803~22.04 level-zero 1.14.0-744~22.04 level-zero 1.14.0-744~22.04 level-zero-dev 1.14.0-744~22.04 level-zero-dev 1.14.0-744~22.04 rsync rsync unzip unzip Python Dependencies torch 2.3.0 torchvision 0.18.0 torchaudio 2.3.0 intel_extension_for_pytorch 2.3.0+cpu Conda Dependencies Container Intel\u00ae Extension for PyTorch XPU Jupyter Image Intel\u00ae Extension for PyTorch XPU Jupyter Image Image Name intel/intel-optimized-pytorch:2.1.20-xpu-idp-jupyter intel/intel-optimized-pytorch:2.1.20-xpu-pip-jupyter Base Image Name intel/intel-optimized-pytorch:2.1.20-xpu-idp-base intel/intel-optimized-pytorch:2.1.20-xpu-pip-base Python Version 3.10 3.10 OS Dependencies Python Dependencies jupyterlab 4.2.0b1 jupyterlab 4.2.0b1 jupyterhub 4.1.5 jupyterhub 4.1.5 notebook 7.2.0b0 notebook 7.2.0b0 jupyter-server-proxy 4.1.2 jupyter-server-proxy 4.1.2 Conda Dependencies <p>{{ Cannot find 'assets/tensorflow.csv' }}</p> CPUPVC PyTorchTensorFlow <p>{{ Cannot find 'assets/pytorch-cpu.csv' }}</p> <p>{{ Cannot find 'assets/tensorflow-cpu.csv' }}</p> FlexMax <p>{{ Cannot find 'assets/flex-pvc.csv' }}</p> <p>{{ Cannot find 'assets/max-pvc.csv' }}</p> Data AnalyticsClassical MLDeep LearningInference Optimization <p>{{ Cannot find 'assets/data_analytics.csv' }}</p> <p>{{ Cannot find 'assets/classical_ml.csv' }}</p> <p>{{ Cannot find 'assets/deep_learning.csv' }}</p> <p>{{ Cannot find 'assets/inference_optimization.csv' }}</p> ServingTransformersGenAI Package Option pip Container Intel\u00ae Extension for PyTorch Serving Image Image Name intel/intel-optimized-pytorch:2.2.0-serving-cpu Base Image Name intel/python:3.10-core Python Version 3.10 OS Dependencies numactl openjdk-17-jdk Python Dependencies torch 2.3.0 torchvision 0.18.0 torchaudio 2.3.0 intel_extension_for_pytorch 2.3.0+cpu captum 0.7.0 cython 3.0.10 pynvml 11.5.0 pyyaml 6.0.1 torch-model-archiver 0.10.0 torch-workflow-archiver 0.2.12 torchserve 0.10.0 torchtext 0.18.0 torchvision 0.18.0 intel_extension_for_pytorch 2.3.0+cpu Conda Dependencies <p>{{ Cannot find 'assets/transformers.csv' }}</p> <p>{{ Cannot find 'assets/genai.csv' }}</p>"},{"location":"roadmap/","title":"Roadmap","text":""},{"location":"roadmap/#q224","title":"Q2'24","text":"<ul> <li>Development in public</li> <li>AI Tools 2024.2 Support</li> <li>Sierra Forest Support</li> </ul>"},{"location":"roadmap/#q324","title":"Q3'24","text":"<ul> <li>Granite Rapid Support</li> <li>CLS Support</li> <li>Intel Developer Cloud Support</li> <li>AI Tools 2024.3/2025.0 Support</li> </ul>"},{"location":"roadmap/#q424","title":"Q4'24","text":"<ul> <li>Gaudi/Falcon Shores Support</li> </ul>"},{"location":"classical-ml/","title":"Intel\u00ae Optimized ML","text":"<p>Intel\u00ae Extension for Scikit-learn* enhances the performance of Scikit-learn* by accelerating the training and inference of machine learning models on Intel\u00ae hardware.</p> <p>XGBoost* is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.</p>"},{"location":"classical-ml/#images","title":"Images","text":"<p>The images below include Intel\u00ae Extension for Scikit-learn* and XGBoost*.</p> Tag(s) Intel SKLearn Scikit-learn XGBoost Dockerfile <code>2024.3.0-pip-base</code>, <code>latest</code> v2024.3.0 v1.4.2 v2.0.3 v0.4.0-Beta <code>2024.2.0-xgboost-2.0.3-pip-base</code>, <code>latest</code> v2024.2.0 v1.4.1 v2.0.3 v0.4.0-Beta <code>scikit-learning-2024.0.0-xgboost-2.0.2-pip-base</code> v2024.0.0 v1.3.2 v2.0.2 v0.3.4 <p>The images below additionally include Jupyter Notebook server:</p> Tag(s) Intel SKLearn Scikit-learn XGBoost Dockerfile <code>2024.3.0-pip-jupyter</code> v2024.3.0 v1.4.2 v2.0.3 v0.4.0-Beta <code>2024.2.0-xgboost-2.0.3-pip-jupyter</code> v2024.2.0 v1.4.1 v2.0.3 v0.4.0-Beta <code>scikit-learning-2024.0.0-xgboost-2.0.2-pip-jupyter</code> v2024.0.0 v1.3.2 v2.0.2 v0.3.4"},{"location":"classical-ml/#run-the-jupyter-container","title":"Run the Jupyter Container","text":"<pre><code>docker run -it --rm \\\n    -p 8888:8888 \\\n    --net=host \\\n    -v $PWD/workspace:/workspace \\\n    -w /workspace \\\n    intel/intel-optimized-ml:2024.2.0-xgboost-2.0.3-pip-jupyter\n</code></pre> <p>After running the command above, copy the URL (something like <code>http://127.0.0.1:$PORT/?token=***</code>) into your browser to access the notebook server.</p>"},{"location":"classical-ml/#images-with-intel-distribution-for-python","title":"Images with Intel\u00ae Distribution for Python*","text":"<p>The images below include [Intel\u00ae Distribution for Python*]:</p> Tag(s) Intel SKLearn Scikit-learn XGBoost Dockerfile <code>2024.3.0-idp-base</code> v2024.3.0 v1.4.2 v2.0.3 v0.4.0-Beta <code>2024.2.0-xgboost-2.0.3-idp-base</code> v2024.2.0 v1.4.1 v2.0.3 v0.4.0-Beta <code>scikit-learning-2024.0.0-xgboost-2.0.2-idp-base</code> v2024.0.0 v1.3.2 v2.0.2 v0.3.4 <p>The images below additionally include Jupyter Notebook server:</p> Tag(s) Intel SKLearn Scikit-learn XGBoost Dockerfile <code>2024.3.0-idp-jupyter</code> v2024.3.0 v1.4.2 v2.0.3 v0.4.0-Beta <code>2024.2.0-xgboost-2.0.3-idp-jupyter</code> v2024.2.0 v1.4.1 v2.0.3 v0.4.0-Beta <code>scikit-learning-2024.0.0-xgboost-2.0.2-idp-jupyter</code> v2024.0.0 v1.3.2 v2.0.2 v0.3.4"},{"location":"classical-ml/#build-from-source","title":"Build from Source","text":"<p>To build the images from source, clone the Intel\u00ae AI Containers repository, follow the main <code>README.md</code> file to setup your environment, and run the following command:</p> <pre><code>cd classical-ml\ndocker compose build ml-base\ndocker compose run ml-base\n</code></pre> <p>You can find the list of services below for each container in the group:</p> Service Name Description <code>ml-base</code> Base image with Intel\u00ae Extension for Scikit-learn* and XGBoost* <code>jupyter</code> Adds Jupyter Notebook server"},{"location":"classical-ml/#license","title":"License","text":"<p>View the License for the Intel\u00ae Distribution for Python.</p> <p>The images below also contain other software which may be under other licenses (such as Pytorch, Jupyter, Bash, etc. from the base).</p> <p>It is the image user's responsibility to ensure that any use of The images below comply with any relevant licenses for all software contained within.</p> <p>* Other names and brands may be claimed as the property of others.</p>"},{"location":"preset/","title":"Intel\u00ae AI Tools Selector Preset Containers","text":"<p>Intel\u00ae AI Tools Selector Preset Containers provides data scientists and developers with environment to perform various data-science tasks such as data analysis, data processing, machine learning and deep learning models training and inference. Each container is equipped with the Python packages and tools suited for each tasks while being powered by Intel\u00ae Distribution For Python. More detail about each container is described in the table below.</p>"},{"location":"preset/#preset-containers","title":"Preset Containers","text":"Preset Container Name Purpose Tools Image Name Data Analytics Perform large scale data analysis Intel\u00ae Distribution For Python, Modin*, Intel\u00ae Dataset Librarian, Intel\u00ae Data Connector <code>intel/data-analytics:latest-py3.9</code><code>intel/data-analytics:latest-py3.10</code> Classical ML Train classical-ml models using scikit, modin and xgboost Intel\u00ae Distribution For Python, Intel\u00ae extension for SciKit Learn, XGBoost*, Modin*,  Intel\u00ae Dataset Librarian, Intel\u00ae Data Connector <code>intel/classical-ml:latest-py3.9</code><code>intel/classical-ml:latest-py3.10</code> Deep Learning Train large scale Deep Learning models with Tensorflow or PyTorch Intel\u00ae Distribution For Python, PyTorch*, Tensorflow*, Intel\u00ae Extension for PyTorch, Intel\u00ae Extension for Tensorflow, Intel\u00ae Optimization for Horovod, Intel\u00ae Dataset Librarian, Intel\u00ae Data Connector, Intel\u00ae Extension for DeepSpeed <code>intel/deep-learning:latest-py3.9</code><code>intel/deep-learning:latest-py3.10</code> Inference Optimization Optimize Deep Learning models for inference using Intel\u00ae Neural Compressor Intel\u00ae Distribution For Python, PyTorch*, Tensorflow*,  Intel\u00ae Extension for PyTorch, Intel\u00ae Extension for Tensorflow, Intel\u00ae Neural Compressor, Intel\u00ae Dataset Librarian, Intel\u00ae Data Connector <code>intel/inference-optimization:latest-py3.9</code><code>intel/inference-optimization:latest-py3.10</code>"},{"location":"preset/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Make sure docker is installed on the machine. Follow the instruction here to install docker engine on a host machine.</p> </li> <li> <p>Pull a Preset Container of your choice from the Intel\u00ae AI Tools Selector or from the table. The commands below use the <code>deep-learning</code> preset as an example.</p> </li> </ol> <pre><code>docker pull intel/deep-learning:latest-py3.9\n</code></pre>"},{"location":"preset/#run-preset-container","title":"Run Preset Container","text":"<p>There are 3 modes to run these containers:</p> <ul> <li>Interactive</li> <li>Jupyter</li> <li>Multi-Node Distributed Training (Deep Learning and Inference Optimization)</li> </ul> <p>Note</p> <p>Modify the commands below to fit your use case, especially the image, environment variables, and GPU device path.</p>"},{"location":"preset/#run-in-interactive-mode","title":"Run in Interactive Mode","text":"<p>This mode allows running the container in an interactive shell. This enables the ability to interact with the container's bash shell. Below is the command to start the container in interactive mode:</p>"},{"location":"preset/#run-on-cpu","title":"Run on CPU","text":"<pre><code>docker run -it --rm \\\n    --shm-size=12G \\\n    -v ${PWD}:/home/dev/workdir \\\n    intel/deep-learning:latest-py3.9 bash\n</code></pre> <p>Note</p> <p>Certain applications use shared memory to share data between processes. But the default shared memory segment size is 64M for docker containers, and is not enough for multithreaded applications(Ex. Modin). Docker recommends increasing shared memory size using <code>--shm-size</code>.</p>"},{"location":"preset/#run-on-gpu","title":"Run on GPU","text":"<p>Find your machine's <code>RENDER</code> and <code>VIDEO</code> group values to enable Intel\u00ae Flex/Max GPU.</p> <pre><code>RENDER=$(getent group render | sed -E 's,^render:[^:]*:([^:]*):.*$,\\1,')\nVIDEO=$(getent group video | sed -E 's,^video:[^:]*:([^:]*):.*$,\\1,')\ntest -z \"$RENDER\" || RENDER_GROUP=\"--group-add ${RENDER}\"\ntest -z \"$VIDEO\" || VIDEO_GROUP=\"--group-add ${VIDEO}\"\n</code></pre> <pre><code>docker run -it --rm \\\n    ${RENDER_GROUP} \\\n    ${VIDEO_GROUP} \\\n    --device=/dev/dri \\\n    --shm-size=12G \\\n    -v ${PWD}:/home/dev/workdir \\\n    -v /dev/dri/by-path:/dev/dri/by-path \\\n    intel/deep-learning:latest-py3.9 bash\n</code></pre> <p>Note</p> <p>Certain applications use shared memory to share data between processes. But the default shared memory segment size is 64M for docker containers, and is not enough for multithreaded applications(Ex. Modin). Docker recommends increasing shared memory size using <code>--shm-size</code>.</p>"},{"location":"preset/#next-steps","title":"Next Steps","text":"<ol> <li> <p>For Deep Learning and Inference Optimization containers there will be separate conda environments for each AI framework: <code>pytorch-cpu</code>, <code>pytorch-gpu</code> and <code>tensorflow</code>. Use the command below to activate one environment:</p> <pre><code>conda activate &lt;env-name&gt;\n</code></pre> </li> <li> <p>Select a test from the <code>sample-tests</code> folder and run it using the following command as an example:</p> <pre><code>bash sample-tests/onnx/run.sh\n# or if no bash script is found\npython sample-tests/intel_extension_for_tensorflow/test_itex.py\n</code></pre> </li> </ol>"},{"location":"preset/#run-using-jupyter-notebook","title":"Run using Jupyter Notebook","text":"<p>This mode launches a jupyterlab notebook server. The command below will start the jupyterlab server which can be accessed from a web browser. Each container includes jupyter kernel to enable conda environment in jupyter notebook. The port for this server is <code>8888</code> and is exposed by default when you run the container.</p> <p>Note</p> <p>When launching a jupyter notebook server this way, docker assigns a network such that the container can communicate with other applications like a web browser. By default docker launches containers with the <code>bridge</code> network, but if you are trying to access this server from a machine you are <code>ssh</code>'ing into, change the network mode with the <code>--net=host</code> flag and ensure you are local port forwarding with <code>ssh -L 8888:8888</code>.</p>"},{"location":"preset/#run-on-jupyter-cpu","title":"Run on Jupyter CPU","text":"<pre><code>docker run -it --rm \\\n    --shm-size=12G \\\n    -v ${PWD}:/home/dev/workdir \\\n    intel/deep-learning:latest-py3.9\n</code></pre> <p>Note</p> <p>Certain applications use shared memory to share data between processes. But the default shared memory segment size is 64M for docker containers, and is not enough for multithreaded applications(Ex. Modin). Docker recommends increasing shared memory size using <code>--shm-size</code>.</p>"},{"location":"preset/#run-on-jupyter-gpu","title":"Run on Jupyter GPU","text":"<p>Find your machine's <code>RENDER</code> and <code>VIDEO</code> group values to enable Intel\u00ae Flex/Max GPU.</p> <pre><code>RENDER=$(getent group render | sed -E 's,^render:[^:]*:([^:]*):.*$,\\1,')\nVIDEO=$(getent group video | sed -E 's,^video:[^:]*:([^:]*):.*$,\\1,')\ntest -z \"$RENDER\" || RENDER_GROUP=\"--group-add ${RENDER}\"\ntest -z \"$VIDEO\" || VIDEO_GROUP=\"--group-add ${VIDEO}\"\n</code></pre> <pre><code>docker run -it --rm \\\n    ${RENDER_GROUP} \\\n    ${VIDEO_GROUP} \\\n    --device=/dev/dri \\\n    --shm-size=12G \\\n    -v ${PWD}:/home/dev/workdir \\\n    -v /dev/dri/by-path:/dev/dri/by-path \\\n    intel/deep-learning:latest-py3.9\n</code></pre> <p>Note</p> <p>Certain applications use shared memory to share data between processes. But the default shared memory segment size is 64M for docker containers, and is not enough for multithreaded applications(Ex. Modin). Docker recommends increasing shared memory size using <code>--shm-size</code>.</p>"},{"location":"preset/#next-steps_1","title":"Next Steps","text":"<ol> <li> <p>After running this command the terminal should display an output similar to displayed below in the image  The server address together with the port set can be used to connect to the jupyter server in a web browser. For example <code>http://127.0.0.1:8888</code>. The token displayed after the <code>token=</code> can be used as a password to login into the server. For example in the image displayed above the token is <code>b66e74a85bc2570bf15782e5124c933c3a4ddabd2cf2d7d3</code>.</p> </li> <li> <p>Select a notebook sample from the Overview notebook found in directory you launched the server with. In this example, the <code>intel/deep-learning</code> container has a notebook titled <code>Deep_Learning_Samples_Overview.ipynb</code> when launched in jupyter mode.</p> </li> <li> <p>After selecting a notebook sample, select the preset kernel found in the dropdown menu presented when loading the notebook. For Deep Learning and Inference Optimization containers there will be multiple kernels, one for each framework: <code>pytorch</code>, <code>pytorch-gpu</code>, and <code>tensorflow</code>.</p> </li> </ol>"},{"location":"preset/#advanced-jupyter-server-configuration","title":"Advanced Jupyter Server Configuration","text":"<p>Modify your notebook server command by using the default example below to change the network (port/ip) and security (privilege) settings by appending it to the docker run commands above:</p> <pre><code>docker run ... intel/deep-learning:latest-py3.9 \\\n    bash -c \"jupyter notebook --notebook-dir=~/jupyter \\\n            --port 8888 \\\n            --ip 0.0.0.0 \\\n            --no-browser \\\n            --allow-root\"\n</code></pre>"},{"location":"preset/#run-in-multi-node-distributed-mode-advanced","title":"Run in Multi-Node Distributed Mode [Advanced]","text":"<p>You can follow the instructions provided for Tensorflow and PyTorch along with the Deep Learning or Inference Optimization presets using your preferred framework.</p>"},{"location":"preset/#troubleshooting-and-support","title":"Troubleshooting and Support","text":"<p>If you face some issue in using the container you can find more information on how to troubleshoot here. If you need more help feel free to submit an issue.</p> <ul> <li>Other names and brands may be claimed as the property of others. Trademarks</li> </ul>"},{"location":"python/","title":"Intel\u00ae Distribution for Python","text":"<p>Intel\u00ae Distribution for Python enhances performance and can improve your program speed from 10 to 100 times faster. It is a Python distribution that includes the Intel\u00ae Math Kernel Library (oneMKL) and other Intel performance libraries to enable near-native performance through acceleration of core numerical and machine learning packages.</p> <p>Intel\u00ae Distribution for Python is available as part of the Intel\u00ae oneAPI Base Toolkit.</p>"},{"location":"python/#images","title":"Images","text":"<p>The images below include variations for only the core packages in the Intel\u00ae Distribution for Python installation, or all of the packages.</p> Tag(s) IDP <code>3.10-full</code>, <code>latest</code> <code>2024.1.0</code> <code>3.10-core</code> <code>2024.1.0</code>"},{"location":"python/#build-from-source","title":"Build from Source","text":"<p>To build the images from source, clone the Intel\u00ae AI Containers repository, follow the main <code>README.md</code> file to setup your environment, and run the following command:</p> <pre><code>cd python\ndocker compose build idp\ndocker compose run idp\n</code></pre> <p>You can find the list of services below for each container in the group:</p> Service Name Description <code>idp</code> Base image with Intel\u00ae Distribution for Python <code>pip</code> Equivalent python image without Intel\u00ae Distribution for Python"},{"location":"python/#license","title":"License","text":"<p>View the License for the Intel\u00ae Distribution for Python.</p> <p>It is the image user's responsibility to ensure that any use of The images below comply with any relevant licenses for all software contained within.</p> <p>* Other names and brands may be claimed as the property of others.</p>"},{"location":"pytorch/","title":"Intel\u00ae Extension for Pytorch*","text":"<p>Intel\u00ae Extension for PyTorch* extends PyTorch* with up-to-date feature optimizations for an extra performance boost on Intel hardware.</p> <p>On Intel CPUs optimizations take advantage of the following instuction sets:</p> <ul> <li>Intel\u00ae Advanced Matrix Extensions (Intel\u00ae AMX)</li> <li>Intel\u00ae Advanced Vector Extensions 512 (Intel\u00ae AVX-512)</li> <li>Vector Neural Network Instructions (VNNI)</li> </ul> <p>On Intel GPUs Intel\u00ae Extension for PyTorch* provides easy GPU acceleration through the PyTorch* <code>xpu</code> device. The following Intel GPUs are supported:</p> <ul> <li>Intel\u00ae Arc\u2122 A-Series Graphics</li> <li>Intel\u00ae Data Center GPU Flex Series</li> <li>Intel\u00ae Data Center GPU Max Series</li> </ul> <p>Images available here start with the Ubuntu* 22.04 base image with Intel\u00ae Extension for PyTorch* built for different use cases as well as some additional software. The Python Dockerfile is used to generate The images below at https://github.com/intel/ai-containers.</p> <p>Note: There are two dockerhub repositories (<code>intel/intel-extension-for-pytorch</code> and <code>intel/intel-optimized-pytorch</code>) that are routinely updated with the latest images, however, some legacy images have not be published to both repositories.</p>"},{"location":"pytorch/#xpu-images","title":"XPU images","text":"<p>The images below include support for both CPU and GPU optimizations:</p> Tag(s) Pytorch IPEX Driver Dockerfile <code>2.1.30-xpu</code> v2.1.0 v2.1.30+xpu 803 v0.4.0-Beta <code>2.1.20-xpu</code> v2.1.0 v2.1.20+xpu 803 v0.3.4 <code>2.1.10-xpu</code> v2.1.0 v2.1.10+xpu 736 v0.2.3 <code>xpu-flex-2.0.110-xpu</code> v2.0.1 v2.0.110+xpu 647 v0.1.0 <pre><code>docker run -it --rm \\\n    --device /dev/dri \\\n    -v /dev/dri/by-path:/dev/dri/by-path \\\n    --ipc=host \\\n    intel/intel-extension-for-pytorch:2.1.30-xpu\n</code></pre> <p>The images below additionally include Jupyter Notebook server:</p> Tag(s) Pytorch IPEX Driver Jupyter Port Dockerfile <code>xpu-jupyter</code> v2.1.0 v2.1.30+xpu 803 <code>8888</code> v0.4.0-Beta <code>2.1.20-xpu-pip-base</code> v2.1.0 v2.1.20+xpu 803 <code>8888</code> v0.3.4 <code>2.1.10-xpu-pip-base</code> v2.1.0 v2.1.10+xpu 736 <code>8888</code> v0.3.4"},{"location":"pytorch/#run-the-xpu-jupyter-container","title":"Run the XPU Jupyter Container","text":"<pre><code>docker run -it --rm \\\n    -p 8888:8888 \\\n    --net=host \\\n    --device /dev/dri \\\n    -v /dev/dri/by-path:/dev/dri/by-path \\\n    --ipc=host \\\n    intel/intel-extension-for-pytorch:xpu-jupyter\n</code></pre> <p>After running the command above, copy the URL (something like <code>http://127.0.0.1:$PORT/?token=***</code>) into your browser to access the notebook server.</p>"},{"location":"pytorch/#cpu-only-images","title":"CPU only images","text":"<p>The images below are built only with CPU optimizations (GPU acceleration support was deliberately excluded):</p> Tag(s) Pytorch IPEX Dockerfile <code>2.3.0-pip-base</code>, <code>latest</code> v2.3.0 v2.3.0+cpu v0.4.0-Beta <code>2.2.0-pip-base</code> v2.2.0 v2.2.0+cpu v0.3.4 <code>2.1.0-pip-base</code> v2.1.0 v2.1.0+cpu v0.2.3 <code>2.0.0-pip-base</code> v2.0.0 v2.0.0+cpu v0.1.0"},{"location":"pytorch/#run-the-cpu-container","title":"Run the CPU Container","text":"<pre><code>docker run -it --rm intel/intel-extension-for-pytorch:latest\n</code></pre> <p>The images below additionally include Jupyter Notebook server:</p> Tag(s) Pytorch IPEX Dockerfile <code>2.3.0-pip-jupyter</code> v2.3.0 v2.3.0+cpu v0.4.0-Beta <code>2.2.0-pip-jupyter</code> v2.2.0 v2.2.0+cpu v0.3.4 <code>2.1.0-pip-jupyter</code> v2.1.0 v2.1.0+cpu v0.2.3 <code>2.0.0-pip-jupyter</code> v2.0.0 v2.0.0+cpu v0.1.0 <pre><code>docker run -it --rm \\\n    -p 8888:8888 \\\n    --net=host \\\n    -v $PWD/workspace:/workspace \\\n    -w /workspace \\\n    intel/intel-extension-for-tensorflow:xpu-jupyter\n</code></pre> <p>After running the command above, copy the URL (something like <code>http://127.0.0.1:$PORT/?token=***</code>) into your browser to access the notebook server.</p> <p>The images below additionally include Intel\u00ae oneAPI Collective Communications Library (oneCCL) and Neural Compressor (INC):</p> Tag(s) Pytorch IPEX oneCCL INC Dockerfile <code>2.3.0-pip-multinode</code> v2.3.0 v2.3.0+cpu v2.3.0 v2.5.1 v0.4.0-Beta <code>2.2.0-pip-multinode</code> v2.2.0 v2.2.0+cpu v2.2.0 v2.4.1 v0.3.4 <code>2.1.0-pip-mulitnode</code> v2.1.0 v2.1.0+cpu v2.1.0 v2.3.1 v0.2.3 <code>2.0.0-pip-multinode</code> v2.0.0 v2.0.0+cpu v2.0.0 v2.1.1 v0.1.0 <p>The images below are TorchServe* with CPU Optimizations:</p> Tag(s) Pytorch IPEX Dockerfile <code>2.3.0-serving-cpu</code> v2.3.0 v2.3.0+cpu v0.4.0-Beta <code>2.2.0-serving-cpu</code> v2.2.0 v2.2.0+cpu v0.3.4 <p>For more details, follow the procedure in the TorchServe instructions.</p>"},{"location":"pytorch/#cpu-only-images-with-intel-distribution-for-python","title":"CPU only images with Intel\u00ae Distribution for Python*","text":"<p>The images below are built only with CPU optimizations (GPU acceleration support was deliberately excluded) and include Intel\u00ae Distribution for Python*:</p> Tag(s) Pytorch IPEX Dockerfile <code>2.3.0-idp-base</code> v2.3.0 v2.3.0+cpu v0.4.0-Beta <code>2.2.0-idp-base</code> v2.2.0 v2.2.0+cpu v0.3.4 <code>2.1.0-idp-base</code> v2.1.0 v2.1.0+cpu v0.2.3 <code>2.0.0-idp-base</code> v2.0.0 v2.0.0+cpu v0.1.0 <p>The images below additionally include Jupyter Notebook server:</p> Tag(s) Pytorch IPEX Dockerfile <code>2.3.0-idp-jupyter</code> v2.3.0 v2.3.0+cpu v0.4.0-Beta <code>2.2.0-idp-jupyter</code> v2.2.0 v2.2.0+cpu v0.3.4 <code>2.1.0-idp-jupyter</code> v2.1.0 v2.1.0+cpu v0.2.3 <code>2.0.0-idp-jupyter</code> v2.0.0 v2.0.0+cpu v0.1.0 <p>The images below additionally include Intel\u00ae oneAPI Collective Communications Library (oneCCL) and Neural Compressor (INC):</p> Tag(s) Pytorch IPEX oneCCL INC Dockerfile <code>2.3.0-idp-multinode</code> v2.3.0 v2.3.0+cpu v2.3.0 v2.5.1 v0.4.0-Beta <code>2.2.0-idp-multinode</code> v2.2.0 v2.2.0+cpu v2.2.0 v2.4.1 v0.3.4 <code>2.1.0-idp-mulitnode</code> v2.1.0 v2.1.0+cpu v2.1.0 v2.3.1 v0.2.3 <code>2.0.0-idp-multinode</code> v2.0.0 v2.0.0+cpu v2.0.0 v2.1.1 v0.1.0"},{"location":"pytorch/#build-from-source","title":"Build from Source","text":"<p>To build the images from source, clone the Intel\u00ae AI Containers repository, follow the main <code>README.md</code> file to setup your environment, and run the following command:</p> <pre><code>cd pytorch\ndocker compose build ipex-base\ndocker compose run ipex-base\n</code></pre> <p>You can find the list of services below for each container in the group:</p> Service Name Description <code>ipex-base</code> Base image with Intel\u00ae Extension for PyTorch* <code>jupyter</code> Adds Jupyter Notebook server <code>multinode</code> Adds Intel\u00ae oneAPI Collective Communications Library and INC <code>xpu</code> Adds Intel GPU Support <code>xpu-jupyter</code> Adds Jupyter notebook server to GPU image <code>serving</code> TorchServe*"},{"location":"pytorch/#license","title":"License","text":"<p>View the License for the Intel\u00ae Extension for PyTorch*.</p> <p>The images below also contain other software which may be under other licenses (such as Pytorch, Jupyter, Bash, etc. from the base).</p> <p>It is the image user's responsibility to ensure that any use of The images below comply with any relevant licenses for all software contained within.</p> <p>* Other names and brands may be claimed as the property of others.</p>"},{"location":"pytorch/serving/","title":"TorchServe","text":"<p>TorchServe is a performant, flexible and easy to use tool for serving PyTorch models in production.</p>"},{"location":"pytorch/serving/#configuration","title":"Configuration","text":"<p>Setting up TorchServe for your production application may require additional steps depending on the type of model you are serving and how that model is served.</p>"},{"location":"pytorch/serving/#archive-model","title":"Archive Model","text":"<p>The Torchserve Model Archiver is a command-line tool found in the torchserve container as well as on pypi. This process is very similar for the TorchServe Workflow.</p> <p>Follow the instructions found in the link above depending on whether you are intending to archive a model or a workflow. Use the provided container rather than installing the archiver with the example command below:</p> <pre><code>curl -O https://download.pytorch.org/models/squeezenet1_1-b8a52dc0.pth\ndocker run --rm -it \\\n           -v $PWD:/home/model-server \\\n           intel/intel-optimized-pytorch:2.2.0-serving-cpu \\\n           torch-model-archiver --model-name squeezenet \\\n            --version 1.0 \\\n            --model-file model-archive/model.py \\\n            --serialized-file squeezenet1_1-b8a52dc0.pth \\\n            --handler image_classifier \\\n            --export-path /home/model-server\n</code></pre>"},{"location":"pytorch/serving/#test-model","title":"Test Model","text":"<p>Test Torchserve with the new archived model. The example below is for the squeezenet model.</p> <pre><code># Assuming that the above pre-archived model is in the current working directory\ndocker run -d --rm --name server \\\n          -v $PWD:/home/model-server/model-store \\\n          --net=host \\\n          intel/intel-optimized-pytorch:2.2.0-serving-cpu\n# Verify that the container has launched successfully\ndocker logs server\n# Attempt to register the model and make an inference request\ncurl -X POST \"http://localhost:8081/models?initial_workers=1&amp;synchronous=true&amp;url=squeezenet1_1.mar&amp;model_name=squeezenet\"\ncurl -O https://raw.githubusercontent.com/pytorch/serve/master/docs/images/kitten_small.jpg\ncurl -X POST http://localhost:8080/v2/models/squeezenet/infer -T kitten_small.jpg\n# Stop the container\ndocker container stop server\n</code></pre>"},{"location":"pytorch/serving/#modify-torchserve-config-file","title":"Modify TorchServe Config File","text":"<p>As demonstrated in the above example, models must be registered before they can be used for predictions. The best way to ensure models are pre-registered with ideal settings is to modify the included config file for the torchserve server.</p> <ol> <li> <p>Add your model to the config file</p> <pre><code>...\ncpu_launcher_enable=true\ncpu_launcher_args=--use_logical_core\n\nmodels={\\\n  \"squeezenet\": {\\\n    \"1.0\": {\\\n        \"defaultVersion\": true,\\\n        \"marName\": \"squeezenet1_1.mar\",\\\n        \"minWorkers\": 1,\\\n        \"maxWorkers\": 1,\\\n        \"batchSize\": 1,\\\n        \"maxBatchDelay\": 1\\\n    }\\\n  }\\\n}\n</code></pre> <p>Note</p> <p>Further customization options can be found in the TorchServe Documentation.</p> </li> <li> <p>Test Config File</p> <pre><code># Assuming that the above pre-archived model is in the current working directory\ndocker run -d --rm --name server \\\n          -v $PWD:/home/model-server/model-store \\\n          -v $PWD/config.properties:/home/model-server/config.properties \\\n          --net=host \\\n          intel/intel-optimized-pytorch:2.2.0-serving-cpu\n# Verify that the container has launched successfully\ndocker logs server\n# Check the models list\ncurl -X GET \"http://localhost:8081/models\"\n# Stop the container\ndocker container stop server\n</code></pre> <p>Expected Output:</p> <pre><code>{\n  \"models\": [\n    {\n      \"modelName\": \"squeezenet\",\n      \"modelUrl\": \"squeezenet1_1.mar\"\n    }\n  ]\n}\n</code></pre> </li> </ol>"},{"location":"pytorch/serving/#simple-maas-on-k8s","title":"Simple MaaS on K8s","text":"<p>Using the provided helm chart your model can scale to multiple nodes in Kubernetes (K8s). Once you have set your <code>KUBECONFIG</code> environment variable and can access your cluster, use the below instructions to deploy your model as a service.</p> <ol> <li> <p>Install Helm</p> <pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 &amp;&amp; \\\nchmod 700 get_helm.sh &amp;&amp; \\\n./get_helm.sh\n</code></pre> </li> <li> <p>(Optional) Push TorchServe Image to a Private Registry</p> <p>If you added layers to an existing torchserve container image in a previous step, use <code>docker push</code> to add that image to a private registry that your cluster can access.</p> </li> <li> <p>Set up Model Storage</p> <p>Your model archive file will no longer be accessible from your local environment, so it needs to be added to a PVC using a network storage solution like NFS.</p> </li> <li> <p>Install TorchServe Chart</p> <p>Using the provided Chart README set the variables found in the table to match the expected model storage, cluster type, and model configuration for your service. The example below assumes that a PVC has been created with the squeezenet model found in the root directory of the volume.</p> <pre><code>helm install \\\n    --namespace=&lt;namespace&gt; \\\n    --set deploy.image=intel/intel-optimized-pytorch:2.2.0-serving-cpu \\\n    --set deploy.models='squeezenet=squeezenet1_1.mar' \\\n    --set deploy.storage.pvc.enable=true \\\n    --set deploy.storage.pvc.claimName=squeezenet \\\n    ipex-serving \\\n    ../charts/inference\n</code></pre> </li> <li> <p>Test Service</p> <p>By default the service is a <code>NodePort</code> service, and is accessible from the ip address of any node in your cluster. Find a node ip with <code>kubectl get node -o wide</code> and attempt to communicate with service using the command below:</p> <pre><code>curl -X GET http://&lt;your-node-ip&gt;:30000/ping\ncurl -X GET http://&lt;your-node-ip&gt;:30001/models\n</code></pre> </li> </ol> <p>Note</p> <p>If you are under a network proxy, you may need to unset your <code>http_proxy</code> and <code>no_proxy</code> to communicate with the nodes in your cluster with <code>curl</code>.</p>"},{"location":"pytorch/serving/#next-steps","title":"Next Steps","text":"<p>There are some additional steps that can be taken to prepare your service for your users:</p> <ul> <li>Enable Autoscaling via Prometheus</li> <li>Enable Intel GPU</li> <li>Enable Metrics and Metrics API.</li> <li>Enable Profiling.</li> <li>Export an INT8 Model for IPEX</li> <li>Integrate an Ingress to your service to serve to a hostname rather than an ip address.</li> <li>Integrate MLFlow.</li> <li>Integrate an SSL Certificate in your model config file to serve models securely.</li> </ul>"},{"location":"pytorch/serving/#kserve","title":"KServe","text":"<p>Apply Intel Optimizations to KServe by patching the serving runtimes to use Intel Optimized Serving Containers with <code>kubectl apply -f patch.yaml</code></p> <p>Note</p> <p>You can modify this <code>patch.yaml</code> file to change the serving runtime pod configuration.</p>"},{"location":"pytorch/serving/#create-an-endpoint","title":"Create an Endpoint","text":"<ol> <li> <p>Create a volume with the follow file configuration:</p> <pre><code>my-volume\n\u251c\u2500\u2500 config\n\u2502   \u2514\u2500\u2500 config.properties\n\u2514\u2500\u2500 model-store\n    \u2514\u2500\u2500 my-model.mar\n</code></pre> </li> <li> <p>Modify your TorchServe Server Configuration with a model snapshot like the following:</p> <pre><code>...\nenable_metrics_api=true\nmetrics_mode=prometheus\nmodel_store=/mnt/models/model-store\nmodel_snapshot={\"name\":\"startup.cfg\",\"modelCount\":1,\"models\":{\"mnist\":{\"1.0\":{\"defaultVersion\":true,\"marName\":\"mnist.mar\",\"minWorkers\":1,\"maxWorkers\":5,\"batchSize\":1,\"responseTimeout\":120}}}}\n</code></pre> </li> </ol> <p>The model snapshot MUST contain the keys <code>defaultVersion</code>, <code>marName</code>, <code>minWorkers</code>, <code>maxWorkers</code>, <code>batchSize</code>, and <code>responseTimeout</code>. Even if your model <code>.mar</code> includes those keys.</p> <ol> <li> <p>Create a new endpoint</p> <pre><code>apiVersion: \"serving.kserve.io/v1beta1\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"ipex-torchserve-sample\"\nspec:\n  predictor:\n    model:\n      modelFormat:\n        name: pytorch\n      protocolVersion: v2\n      storageUri: pvc://my-volume\n</code></pre> </li> <li> <p>Test the endpoint</p> <pre><code>curl -v -H \"Host: ${SERVICE_HOSTNAME}\" http://${INGRESS_HOST}:${INGRESS_PORT}/v2/models\n</code></pre> </li> <li> <p>Make a Prediction    Use this python script to convert your input to a bytes format:</p> </li> </ol> <pre><code>import base64\nimport json\nimport argparse\nimport uuid\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"filename\", help=\"converts image to bytes array\", type=str)\nargs = parser.parse_args()\n\nimage = open(args.filename, \"rb\")  # open binary file in read mode\nimage_read = image.read()\nimage_64_encode = base64.b64encode(image_read)\nbytes_array = image_64_encode.decode(\"utf-8\")\nrequest = {\n    \"inputs\": [\n         {\n             \"name\": str(uuid.uuid4()),\n             \"shape\": [-1],\n             \"datatype\": \"BYTES\",\n             \"data\": [bytes_array],\n         }\n    ]\n }\n\n result_file = \"{filename}.{ext}\".format(\n     filename=str(args.filename).split(\".\")[0], ext=\"json\"\n )\n with open(result_file, \"w\") as outfile:\n     json.dump(request, outfile, indent=4, sort_keys=True)\n ```\n\nUsing the script will produce a json file to use as a prediction payload:\n\n```bash\ncurl -v -H \"Host: ${SERVICE_HOSTNAME}\" -X POST \\\nhttp://${INGRESS_HOST}:${INGRESS_PORT}/v2/models/${MODELNAME}/infer \\\n-d @./${PAYLOAD}.json\n</code></pre> <p>Tip</p> <p>You can find your <code>SERVICE_HOSTNAME</code> in the KubeFlow UI with the copy button and removing the <code>http://</code> from the url.</p> <p>Tip</p> <p>You can find your ingress information with <code>kubectl get svc -n istio-system | grep istio-ingressgateway</code> and using the external IP and port mapped to <code>80</code>.</p>"},{"location":"tensorflow/","title":"Intel\u00ae Extension for TensorFlow*","text":"<p>Intel\u00ae Extension for TensorFlow* extends TensorFlow* with up-to-date feature optimizations for an extra performance boost on Intel hardware.</p> <p>Intel\u00ae Extension for TensorFlow* is based on the TensorFlow PluggableDevice interface to bring Intel XPU(GPU, CPU, etc.) devices into TensorFlow* with flexibility for on-demand performance on the following Intel GPUs:</p> <ul> <li>Intel\u00ae Arc\u2122 A-Series Graphics</li> <li>Intel\u00ae Data Center GPU Flex Series</li> <li>Intel\u00ae Data Center GPU Max Series</li> </ul> <p>Note: There are two dockerhub repositories (<code>intel/intel-extension-for-tensorflow</code> and <code>intel/intel-optimized-tensorflow</code>) that are routinely updated with the latest images, however, some legacy images have not be published to both repositories.</p>"},{"location":"tensorflow/#xpu-images","title":"XPU images","text":"<p>The images below include support for both CPU and GPU optimizations:</p> Tag(s) TensorFlow ITEX Driver Dockerfile <code>2.15.0.0-xpu</code>, <code>xpu</code> v2.15.0 v2.15.0.0 803 v0.4.0-Beta <code>2.14.0.1-xpu</code> v2.14.1 v2.14.0.1 736 v0.3.4 <code>2.13.0.0-xpu</code> v2.13.0 v2.13.0.0 647 v0.2.3"},{"location":"tensorflow/#run-the-xpu-container","title":"Run the XPU Container","text":"<pre><code>docker run -it --rm \\\n    --device /dev/dri \\\n    -v /dev/dri/by-path:/dev/dri/by-path \\\n    --ipc=host \\\n    intel/intel-extension-for-tensorflow:xpu\n</code></pre> <p>The images below additionally include Jupyter Notebook server:</p> Tag(s) TensorFlow IPEX Driver Dockerfile <code>xpu-jupyter</code> v2.14.1 v2.14.0.1 736 v0.3.4"},{"location":"tensorflow/#run-the-xpu-jupyter-container","title":"Run the XPU Jupyter Container","text":"<pre><code>docker run -it --rm \\\n    -p 8888:8888 \\\n    --net=host \\\n    --device /dev/dri \\\n    -v /dev/dri/by-path:/dev/dri/by-path \\\n    --ipc=host \\\n    intel/intel-extension-for-tensorflow:xpu-jupyter\n</code></pre> <p>After running the command above, copy the URL (something like <code>http://127.0.0.1:$PORT/?token=***</code>) into your browser to access the notebook server.</p> <p>The images below are TensorFlow* Serving with GPU Optimizations:</p> Tag(s) TensorFlow IPEX <code>2.14.0.1-serving-gpu</code>, <code>serving-gpu</code> v2.14.1 v2.14.0.1 <code>2.13.0.0-serving-gpu</code>, v2.13.0 v2.13.0.0"},{"location":"tensorflow/#run-the-serving-gpu-container","title":"Run the Serving GPU Container","text":"<pre><code>docker run -it --rm \\\n    -p 8500:8500 \\\n    --device /dev/dri \\\n    -v /dev/dri/by-path:/dev/dri/by-path \\\n    -v $PWD/workspace:/workspace \\\n    -w /workspace \\\n    -e MODEL_NAME=&lt;your-model-name&gt; \\\n    -e MODEL_DIR=&lt;your-model-dir&gt; \\\n    intel/intel-extension-for-tensorflow:serving-gpu\n</code></pre> <p>For more details, follow the procedure in the Intel\u00ae Extension for TensorFlow* Serving instructions.</p>"},{"location":"tensorflow/#cpu-only-images","title":"CPU only images","text":"<p>The images below are built only with CPU optimizations (GPU acceleration support was deliberately excluded):</p> Tag(s) TensorFlow ITEX Dockerfile <code>2.15.0-pip-base</code>, <code>latest</code> v2.15.0 v2.15.0.0 v0.4.0-Beta <code>2.14.0-pip-base</code> v2.14.1 v2.14.0.1 v0.3.4 <code>2.13-pip-base</code> v2.13.0 v2.13.0.0 v0.2.3 <p>The images below additionally include Jupyter Notebook server:</p> Tag(s) TensorFlow ITEX Dockerfile <code>2.15.0-pip-jupyter</code> v2.15.0 v2.15.0.0 v0.4.0-Beta <code>2.14.0-pip-jupyter</code> v2.14.1 v2.14.0.1 v0.3.4 <code>2.13-pip-jupyter</code> v2.13.0 v2.13.0.0 v0.2.3"},{"location":"tensorflow/#run-the-cpu-jupyter-container","title":"Run the CPU Jupyter Container","text":"<pre><code>docker run -it --rm \\\n    -p 8888:8888 \\\n    --net=host \\\n    -v $PWD/workspace:/workspace \\\n    -w /workspace \\\n    intel/intel-extension-for-tensorflow:xpu-jupyter\n</code></pre> <p>After running the command above, copy the URL (something like <code>http://127.0.0.1:$PORT/?token=***</code>) into your browser to access the notebook server.</p> <p>The images below additionally include Horovod:</p> Tag(s) Tensorflow ITEX Horovod Dockerfile <code>2.15.0-pip-multinode</code> v2.15.0 v2.15.0.0 v0.28.1 v0.4.0-Beta <code>2.14.0-pip-openmpi-multinode</code> v2.14.1 v2.14.0.1 v0.28.1 v0.3.4 <code>2.13-pip-openmpi-mulitnode</code> v2.13.0 v2.13.0.0 v0.28.0 v0.2.3 <p>The images below are TensorFlow* Serving with CPU Optimizations:</p> Tag(s) TensorFlow ITEX <code>2.14.0.1-serving-cpu</code>, <code>serving-cpu</code> v2.14.1 v2.14.0.1 <code>2.13.0.0-serving-cpu</code> v2.13.0 v2.13.0.0"},{"location":"tensorflow/#run-the-serving-cpu-container","title":"Run the Serving CPU Container","text":"<pre><code>docker run -it --rm \\\n    -p 8500:8500 \\\n    --device /dev/dri \\\n    -v /dev/dri/by-path:/dev/dri/by-path \\\n    -v $PWD/workspace:/workspace \\\n    -w /workspace \\\n    -e MODEL_NAME=&lt;your-model-name&gt; \\\n    -e MODEL_DIR=&lt;your-model-dir&gt; \\\n    intel/intel-extension-for-tensorflow:serving-cpu\n</code></pre> <p>For more details, follow the procedure in the Intel\u00ae Extension for TensorFlow* Serving instructions.</p>"},{"location":"tensorflow/#cpu-only-images-with-intel-distribution-for-python","title":"CPU only images with Intel\u00ae Distribution for Python*","text":"<p>The images below are built only with CPU optimizations (GPU acceleration support was deliberately excluded) and include Intel\u00ae Distribution for Python*:</p> Tag(s) TensorFlow ITEX Dockerfile <code>2.15.0-idp-base</code>, <code>latest</code> v2.15.0 v2.15.0.0 v0.4.0-Beta <code>2.14.0-idp-base</code> v2.14.1 v2.14.0.1 v0.3.4 <code>2.13-idp-base</code> v2.13.0 v2.13.0.0 v0.2.3 <p>The images below additionally include Jupyter Notebook server:</p> Tag(s) TensorFlow ITEX Dockerfile <code>2.15.0-idp-jupyter</code> v2.15.0 v2.15.0.0 v0.4.0-Beta <code>2.14.0-idp-jupyter</code> v2.14.1 v2.14.0.1 v0.3.4 <code>2.13-idp-jupyter</code> v2.13.0 v2.13.0.0 v0.2.3 <p>The images below additionally include Horovod:</p> Tag(s) Tensorflow ITEX Horovod Dockerfile <code>2.15.0-idp-multinode</code> v2.15.0 v2.15.0.0 v0.28.1 v0.4.0-Beta <code>2.14.0-idp-openmpi-multinode</code> v2.14.1 v2.14.0.1 v0.28.1 v0.3.4 <code>2.13-idp-openmpi-mulitnode</code> v2.13.0 v2.13.0.0 v0.28.0 v0.2.3"},{"location":"tensorflow/#build-from-source","title":"Build from Source","text":"<p>To build the images from source, clone the Intel\u00ae AI Containers repository, follow the main <code>README.md</code> file to setup your environment, and run the following command:</p> <pre><code>cd pytorch\ndocker compose build tf-base\ndocker compose run tf-base\n</code></pre> <p>You can find the list of services below for each container in the group:</p> Service Name Description <code>tf-base</code> Base image with Intel\u00ae Extension for TensorFlow* <code>jupyter</code> Adds Jupyter Notebook server <code>multinode</code> Adds Intel\u00ae MPI, Horovod and INC <code>xpu</code> Adds Intel GPU Support <code>xpu-jupyter</code> Adds Jupyter notebook server to GPU image"},{"location":"tensorflow/#license","title":"License","text":"<p>View the License for the Intel\u00ae Extension for TensorFlow*.</p> <p>The images below also contain other software which may be under other licenses (such as TensorFlow, Jupyter, Bash, etc. from the base).</p> <p>It is the image user's responsibility to ensure that any use of The images below comply with any relevant licenses for all software contained within.</p> <p>* Other names and brands may be claimed as the property of others.</p>"}]}
site/classical-ml/index.html:<span class="w">    </span>intel/intel-optimized-ml:2024.2.0-xgboost-2.0.3-pip-jupyter
site/pytorch/index.html:<span class="w">    </span>intel/intel-extension-for-pytorch:2.1.30-xpu
site/pytorch/index.html:<span class="w">    </span>intel/intel-extension-for-pytorch:xpu-jupyter
site/pytorch/index.html:<div class="highlight"><pre><span></span><code>docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span>--rm<span class="w"> </span>intel/intel-extension-for-pytorch:latest
site/pytorch/index.html:<span class="w">    </span>intel/intel-extension-for-tensorflow:xpu-jupyter
site/pytorch/serving/index.html:<span class="w">           </span>intel/intel-optimized-pytorch:2.2.0-serving-cpu<span class="w"> </span><span class="se">\</span>
site/pytorch/serving/index.html:<span class="w">          </span>intel/intel-optimized-pytorch:2.2.0-serving-cpu
site/pytorch/serving/index.html:<span class="w">          </span>intel/intel-optimized-pytorch:2.2.0-serving-cpu
site/pytorch/serving/index.html:<span class="w">    </span>--set<span class="w"> </span>deploy.image<span class="o">=</span>intel/intel-optimized-pytorch:2.2.0-serving-cpu<span class="w"> </span><span class="se">\</span>
site/assets/pytorch.csv:Image Name,intel/intel-optimized-pytorch:2.2.0-idp-base,intel/intel-optimized-pytorch:2.2.0-pip-base
site/assets/pytorch.csv:Base Image Name,intel/python:3.10-core,intel/python:3.10-core
site/assets/pytorch.csv:Image Name,intel/intel-optimized-pytorch:2.2.0-idp-jupyter,intel/intel-optimized-pytorch:2.2.0-pip-jupyter
site/assets/pytorch.csv:Base Image Name,intel/intel-optimized-pytorch:2.2.0-idp-base,intel/intel-optimized-pytorch:2.2.0-pip-base
site/assets/pytorch.csv:Image Name,intel/intel-optimized-pytorch:2.2.0-idp-multinode,intel/intel-optimized-pytorch:2.2.0-pip-multinode
site/assets/pytorch.csv:Base Image Name,intel/intel-optimized-pytorch:2.2.0-idp-base,intel/intel-optimized-pytorch:2.2.0-pip-base
site/assets/pytorch.csv:Image Name,intel/intel-optimized-pytorch:2.1.20-xpu-idp-base,intel/intel-optimized-pytorch:2.1.20-xpu-pip-base
site/assets/pytorch.csv:Base Image Name,intel/python:3.10-core,intel/python:3.10-core
site/assets/pytorch.csv:Image Name,intel/intel-optimized-pytorch:2.1.20-xpu-idp-jupyter,intel/intel-optimized-pytorch:2.1.20-xpu-pip-jupyter
site/assets/pytorch.csv:Base Image Name,intel/intel-optimized-pytorch:2.1.20-xpu-idp-base,intel/intel-optimized-pytorch:2.1.20-xpu-pip-base
site/assets/serving.csv:Image Name,intel/intel-optimized-pytorch:2.2.0-serving-cpu
site/assets/serving.csv:Base Image Name,intel/python:3.10-core
site/assets/python.csv:Image Name,intel/python:3.10-full,intel/python:3.10-core
site/matrix/index.html:<td style="text-align: left;">intel/python:3.10-full</td>
site/matrix/index.html:<td style="text-align: left;">intel/python:3.10-core</td>
site/matrix/index.html:<td style="text-align: left;">intel/intel-optimized-pytorch:2.2.0-idp-base</td>
site/matrix/index.html:<td style="text-align: left;">intel/intel-optimized-pytorch:2.2.0-pip-base</td>
site/matrix/index.html:<td style="text-align: left;">intel/python:3.10-core</td>
site/matrix/index.html:<td style="text-align: left;">intel/python:3.10-core</td>
site/matrix/index.html:<td style="text-align: left;">intel/intel-optimized-pytorch:2.2.0-idp-jupyter</td>
site/matrix/index.html:<td style="text-align: left;">intel/intel-optimized-pytorch:2.2.0-pip-jupyter</td>
site/matrix/index.html:<td style="text-align: left;">intel/intel-optimized-pytorch:2.2.0-idp-base</td>
site/matrix/index.html:<td style="text-align: left;">intel/intel-optimized-pytorch:2.2.0-pip-base</td>
site/matrix/index.html:<td style="text-align: left;">intel/intel-optimized-pytorch:2.2.0-idp-multinode</td>
site/matrix/index.html:<td style="text-align: left;">intel/intel-optimized-pytorch:2.2.0-pip-multinode</td>
site/matrix/index.html:<td style="text-align: left;">intel/intel-optimized-pytorch:2.2.0-idp-base</td>
site/matrix/index.html:<td style="text-align: left;">intel/intel-optimized-pytorch:2.2.0-pip-base</td>
site/matrix/index.html:<td style="text-align: left;">intel/intel-optimized-pytorch:2.1.20-xpu-idp-base</td>
site/matrix/index.html:<td style="text-align: left;">intel/intel-optimized-pytorch:2.1.20-xpu-pip-base</td>
site/matrix/index.html:<td style="text-align: left;">intel/python:3.10-core</td>
site/matrix/index.html:<td style="text-align: left;">intel/python:3.10-core</td>
site/matrix/index.html:<td style="text-align: left;">intel/intel-optimized-pytorch:2.1.20-xpu-idp-jupyter</td>
site/matrix/index.html:<td style="text-align: left;">intel/intel-optimized-pytorch:2.1.20-xpu-pip-jupyter</td>
site/matrix/index.html:<td style="text-align: left;">intel/intel-optimized-pytorch:2.1.20-xpu-idp-base</td>
site/matrix/index.html:<td style="text-align: left;">intel/intel-optimized-pytorch:2.1.20-xpu-pip-base</td>
site/matrix/index.html:<td style="text-align: left;">intel/intel-optimized-pytorch:2.2.0-serving-cpu</td>
site/matrix/index.html:<td style="text-align: left;">intel/python:3.10-core</td>
site/preset/index.html:<td><a href="https://hub.docker.com/r/intel/data-analytics/tags"><code>intel/data-analytics:latest-py3.9</code></a><br /><a href="https://hub.docker.com/r/intel/data-analytics/tags"><code>intel/data-analytics:latest-py3.10</code></a></td>
site/preset/index.html:<td><a href="https://hub.docker.com/r/intel/classical-ml/tags"><code>intel/classical-ml:latest-py3.9</code></a><br /><a href="https://hub.docker.com/r/intel/classical-ml/tags"><code>intel/classical-ml:latest-py3.10</code></a></td>
site/preset/index.html:<td><a href="https://hub.docker.com/r/intel/deep-learning/tags"><code>intel/deep-learning:latest-py3.9</code></a><br /><a href="https://hub.docker.com/r/intel/deep-learning/tags"><code>intel/deep-learning:latest-py3.10</code></a></td>
site/preset/index.html:<td><a href="https://hub.docker.com/r/intel/inference-optimization/tags"><code>intel/inference-optimization:latest-py3.9</code></a><br /><a href="https://hub.docker.com/r/intel/inference-optimization/tags"><code>intel/inference-optimization:latest-py3.10</code></a></td>
site/preset/index.html:<div class="highlight"><pre><span></span><code>docker<span class="w"> </span>pull<span class="w"> </span>intel/deep-learning:latest-py3.9
site/preset/index.html:<span class="w">    </span>intel/deep-learning:latest-py3.9<span class="w"> </span>bash
site/preset/index.html:<span class="w">    </span>intel/deep-learning:latest-py3.9<span class="w"> </span>bash
site/preset/index.html:<span class="w">    </span>intel/deep-learning:latest-py3.9
site/preset/index.html:<span class="w">    </span>intel/deep-learning:latest-py3.9
site/preset/index.html:<div class="highlight"><pre><span></span><code>docker<span class="w"> </span>run<span class="w"> </span>...<span class="w"> </span>intel/deep-learning:latest-py3.9<span class="w"> </span><span class="se">\</span>
tensorflow/serving/README.md:              intel/intel-extension-for-tensorflow:serving-cpu --model_config_file=/models/models.config
tensorflow/serving/README.md:    IMAGE=$(docker ps -aqf "ancestor=intel/intel-extension-for-tensorflow:serving-cpu")
tensorflow/docker-compose.yaml:        org.opencontainers.base.name: "intel/python:3.10-core"
tensorflow/docker-compose.yaml:        org.opencontainers.base.name: "intel/python:3.10-core"
tensorflow/README.md:    intel/intel-extension-for-tensorflow:xpu
tensorflow/README.md:    intel/intel-extension-for-tensorflow:2.15.0.1-xpu-pip-jupyter
tensorflow/README.md:    intel/intel-extension-for-tensorflow:serving-gpu
tensorflow/README.md:    intel/intel-extension-for-tensorflow:2.15.1-pip-jupyter
tensorflow/README.md:            intel/intel-optimized-tensorflow:2.15.1-pip-multinode \
tensorflow/README.md:            intel/intel-optimized-tensorflow:2.15.1-pip-multinode \
tensorflow/README.md:    intel/intel-extension-for-tensorflow:serving-cpu
third_party/tpp_intel-classical-ml-pip-jupyter.txt:intel/classical-ml:pip-jupyter container Third Party Programs File
third_party/tpp_intel-inference-optimization-py3.9.txt:intel/inference-optimization:2023.2-py3.9 container Third Party Programs File
third_party/tpp_intel-python-py310-full.txt:intel/python:py310-full container Third Party Programs File
third_party/tpp_intel-intel-optimized-tensorflow-idp-openmpi-multinode.txt:intel/intel-optimized-tensorflow:2.13-idp-openmpi-multinode container Third Party Programs File
third_party/tpp_intel-intel-extension-for-pytorch-xpu-idp-base.txt:intel/intel-extension-for-pytorch:2.1.10-xpu-idp-base container Third Party Programs File
third_party/tpp_intel-intel-extension-for-pytorch-idp-jupyter.txt:intel/intel-extension-for-pytorch:2.2.0-idp-jupyter container Third Party Programs File
third_party/tpp_intel-intel-extension-for-pytorch-pip-base.txt:intel/intel-extension-for-pytorch:2.3.0-pip-base container Third Party Programs File
third_party/tpp_intel-inference-optimization-py3.10.txt:intel/inference-optimization:2023.2-py3.10 container Third Party Programs File
third_party/tpp_intel-data-analytics-py3.9.txt:intel/data-analytics:2023.2-py3.9 container Third Party Programs File
third_party/tpp_intel-classical-ml-py3.10.txt:intel/classical-ml:2023.2-py3.10 container Third Party Programs File
third_party/tpp_intel-classical-ml-idp-jupyter.txt:intel/classical-ml:idp-jupyter container Third Party Programs File
third_party/tpp_intel-intel-extension-for-pytorch-pip-multinode.txt:intel/intel-extension-for-pytorch:2.0.0-pip-multinode container Third Party Programs File
third_party/tpp_intel-intel-extension-for-pytorch-pip-jupyter.txt:intel/intel-extension-for-pytorch:2.3.0-pip-jupyter container Third Party Programs File
third_party/tpp_intel-intel-optimized-tensorflow-idp-base.txt:intel/intel-optimized-tensorflow:2.13-idp-base container Third Party Programs File
third_party/tpp_intel-intel-optimized-tensorflow-pip-openmpi-multinode.txt:intel/intel-optimized-tensorflow:2.13-pip-openmpi-multinode container Third Party Programs File
third_party/tpp_intel-intel-extension-for-pytorch-idp-multinode.txt:intel/intel-extension-for-pytorch:2.0.0-idp-multinode container Third Party Programs File
third_party/tpp_intel-classical-ml-idp-base.txt:intel/classical-ml:idp-base container Third Party Programs File
third_party/tpp_intel-python-py310-core.txt:intel/python:py310-core container Third Party Programs File
third_party/tpp_intel-intel-optimized-tensorflow-pip-base.txt:intel/intel-optimized-tensorflow:2.13-pip-base container Third Party Programs File
third_party/tpp_intel-classical-ml-py3.9.txt:intel/classical-ml:2023.2-py3.9 container Third Party Programs File
third_party/tpp_intel-intel-extension-for-pytorch-xpu.txt:intel/intel-extension-for-pytorch:2.1.30-xpu container Third Party Programs File
third_party/tpp_intel-intel-extension-for-pytorch-serving-cpu-kserve.txt:intel/intel-extension-for-pytorch:2.2.0-serving-cpu-kserve container Third Party Programs File
third_party/tpp_intel-intel-extension-for-pytorch-xpu-pip-jupyter.txt:intel/intel-extension-for-pytorch:2.1.20-xpu-pip-jupyter container Third Party Programs File
third_party/tpp_intel-classical-ml-pip-base.txt:intel/classical-ml:pip-base container Third Party Programs File
third_party/tpp_intel-data-analytics-py3.10.txt:intel/data-analytics:2023.2-py3.10 container Third Party Programs File
third_party/tpp_intel-intel-optimized-tensorflow-idp-jupyter.txt:intel/intel-optimized-tensorflow:2.13-idp-jupyter container Third Party Programs File
third_party/tpp_intel-deep-learning-py3.10.txt:intel/deep-learning:2023.2-py3.10 container Third Party Programs File
third_party/tpp_intel-intel-extension-for-pytorch-xpu-pip-base.txt:intel/intel-extension-for-pytorch:2.1.20-xpu-pip-base container Third Party Programs File
third_party/tpp_intel-intel-extension-for-pytorch-xpu-idp-jupyter.txt:intel/intel-extension-for-pytorch:2.1.20-xpu-idp-jupyter container Third Party Programs File
third_party/tpp_intel-intel-optimized-pytorch-serving-cpu.txt:intel/intel-optimized-pytorch:2.1.0-serving-cpu container Third Party Programs File
third_party/tpp_intel-deep-learning-py3.9.txt:intel/deep-learning:2023.2-py3.9 container Third Party Programs File
third_party/tpp_intel-intel-extension-for-pytorch-idp-base.txt:intel/intel-extension-for-pytorch:2.0.0-idp-base container Third Party Programs File
third_party/tpp_intel-intel-optimized-tensorflow-pip-jupyter.txt:intel/intel-optimized-tensorflow:2.13-pip-jupyter container Third Party Programs File
workflows/charts/torchserve/values.yaml:  image: intel/intel-optimized-pytorch:2.3.0-serving-cpu
workflows/charts/torchserve/README.md:| deploy.image | string | `"intel/intel-optimized-pytorch:2.3.0-serving-cpu"` | Intel Optimized torchserve image |
workflows/charts/huggingface-llm/README.md:is used as the DDP backend. The `intel/intel-optimized-pytorch:2.3.0-pip-multinode` base image already includes these
workflows/charts/huggingface-llm/README.md:An image has been published to DockerHub (`intel/ai-workflows:torch-2.3.0-huggingface-multinode-py3.10`) with
workflows/charts/huggingface-llm/README.md.gotmpl:is used as the DDP backend. The `intel/intel-optimized-pytorch:2.3.0-pip-multinode` base image already includes these
workflows/charts/huggingface-llm/README.md.gotmpl:An image has been published to DockerHub (`intel/ai-workflows:torch-2.3.0-huggingface-multinode-py3.10`) with
workflows/README.md:| `intel/intel-optimized-pytorch:2.3.0-pip-multinode` | CPU | [Distributed LLM Fine Tuning with Kubernetes] | Demonstrates using Hugging Face Transformers with Intel® Xeon® Scalable Processors to fine tune LLMs with multiple nodes from a Kubernetes cluster. The example includes a LLM fine tuning script, Dockerfile, and Helm chart. |
workflows/README.md:| `intel/intel-optimized-pytorch:2.3.0-serving-cpu` | CPU | [TorchServe* with Kubernetes] | Demonstrates using TorchServe* with Intel® Xeon® Scalable Processors to serve models on multinodes nodes from a Kubernetes cluster. The example includes a Helm chart. |
